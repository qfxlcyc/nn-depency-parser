import tensorflow as tf
import numpy as np
from tensorflow.contrib import learn

import argparse

from tensorflow.python.lib.io import file_io
from six.moves import cPickle

import re
import os

import trainer.data_helper as data_helper

import pandas as pd
from pandas.compat import StringIO


labels = ["shift", "left-arc", "right-arc"]

def load_data(path):
    """ example of a record:
    1   The _   DET DT  _   4   det _   _
    2   complex _   ADJ JJ  _   4   amod    _   _
    3   financing   _   NOUN    NN  _   4   compound    _   _
    4   plan    _   NOUN    NN  _   10  nsubj   _   _
    5   in  _   ADP IN  _   9   case    _   _
    6   the _   DET DT  _   9   det _   _
    7   S&L _   NOUN    NN  _   9   compound    _   _
    8   bailout _   NOUN    NN  _   9   compound    _   _
    9   law _   NOUN    NN  _   4   nmod    _   _
    10  includes    _   VERB    VBZ _   0   root    _   _
    11  raising _   VERB    VBG _   10  xcomp   _   _
    12  $   _   SYM $   _   11  dobj    _   _
    13  30  _   NUM CD  _   14  compound    _   _
    14  billion _   NUM CD  _   12  nummod  _   _
    15  from    _   ADP IN  _   16  case    _   _
    16  debt    _   NOUN    NN  _   11  nmod    _   _
    17  issued  _   VERB    VBN _   16  acl _   _
    18  by  _   ADP IN  _   22  case    _   _
    19  the _   DET DT  _   22  det _   _
    20  newly   _   ADV RB  _   21  advmod  _   _
    21  created _   VERB    VBN _   22  amod    _   _
    22  RTC _   PROPN   NNP _   17  nmod    _   _
    23  .   _   PUNCT   .   _   10  punct   _   _
    """
    with open(path, 'r') as f:
        data = f.read().split('\n\n')
        data = [record.split('\n') for record in data]
        data = [[l.split('\t') for l in record] for record in data]
        
        sentense, pos, arc_labels, trans = [], [], [], []
        for record in data:
            min_len = min(len(l) for l in record)
            if min_len < 7:
                print "record with invalid line"
                print record
                continue
            sentense.append([l[1] for l in record])
            pos.append([l[4] for l in record])
            arc_labels.append([l[7] for l in record])
            trans.append([(int(l[0]), int(l[6])) for l in record])
        return sentense, pos, arc_labels, trans

def load_vocab(path):
    with open(path, 'r') as f:
        data = f.readlines()
        vocab = [l.split('\t')[0] for l in data]
        return vocab


def generate_labels_from_arcs(record):
    def is_projective(c, h, lc, lh):
        return c >= lh or h <= lh

    stacks = []
    labels = []
    for c, h in record:
        labels.append(0)
        if not stacks: 
            stacks.append([c, h])
            continue
        while stacks:
            lc, lh = stacks[-1]
            if not is_projective(c, h, lc, lh):
                print "Record is not projective, skip"
                # print [(c, h), (lc, lh)]
                print record
                return []
            if c >= lh > 0:
                labels.append(1)
                stacks.pop()
            else:
                break
        if c > h > 0:
            labels.append(2)
        else:
            stacks.append([c, h])


    if len(stacks) != 1:
        print record
        raise Exception("Error in creating labels for record. Should have a remaining left-arc to root")
    labels.append(2)
    return labels


def create_features(parser, sentense, pos, arc_labels):
    """create model inputs from parser's current configuration"""

    def n_top_nodes(l, n=3):
        return l[:n] + [data_helper.Node() for _ in xrange(n-len(l))]

    def get_value_by_node_index(values, nodes, default=-1):
        """return values indexed by indexes stored in nodes, with default value filled in if the index is out of range"""
        return [values[n.val] if 0 <= n.val < len(values) else default for n in nodes]

    feature_nodes = []
    stack_top3_nodes = n_top_nodes(list(reversed(parser.stack)), 3)
    buffer_top3_nodes = n_top_nodes(list(reversed(parser.buffer)), 3)
    # print "stack top 3: %s" % (','.join(str(n.val) for n in stack_top3_nodes))
    # print "buffer top 3: %s" % (','.join(str(n.val) for n in buffer_top3_nodes))
    for node in stack_top3_nodes[:2]:
        if not node: 
            feature_nodes += [data_helper.Node() for _ in xrange(6)]
            continue
        lc1, lc2, rc1, rc2 = node.nth_child(1, left=True), node.nth_child(2, left=True), node.nth_child(1), node.nth_child(2)
        lc1_lc1, rc1_rc1 = lc1.nth_child(1, left=True), rc1.nth_child(1)
        feature_nodes += [lc1, lc2, rc1, rc2, lc1_lc1, rc1_rc1]
        # print "feature nodes generated by node %s: %s" % (str(node.val), ','.join(str(n.val) for n in [lc1, lc2, rc1, rc2, lc1_lc1, rc1_rc1]))

    # 12 arc features
    arc_feat = get_value_by_node_index(arc_labels, feature_nodes)

    feature_nodes += stack_top3_nodes + buffer_top3_nodes

    # 18 pos and word feature_nodes
    pos_feat = get_value_by_node_index(pos, feature_nodes)
    sentense_feat = get_value_by_node_index(sentense, feature_nodes)
    # print [n.val for n in feature_nodes]
    
    return sentense_feat + pos_feat


class Preprocess:

    def __init__(self, data_path, output_path, arc_class_path, pos_class_path, emb_path, vocab_path, train):
        self.data_path = data_path
        self.output_path = output_path
        self.arc_class_path = arc_class_path
        self.pos_class_path = pos_class_path
        self.emb_path = emb_path
        self.vocab_path = vocab_path
        self.train = train

    def run(self):
        sentenses, pos, arc_labels, trans = load_data(self.data_path)
        if self.train:
            arc_class = write_class(arc_labels, self.arc_class_path)
            pos_class = write_class(pos, self.pos_class_path)
        else:
            arc_class = data_helper.load_class(self.arc_class_path)
            pos_class = data_helper.load_class(self.pos_class_path)

        
        print "convert words to indices"
        vocab_processor = get_vocab_processor(self.emb_path, self.vocab_path, self.train)
        sentence_ids = transform_sentences(sentenses, vocab_processor)

        print "convert arc labels and pos tags to indices"
        arc_ids = transform_to_id(arc_labels, arc_class)
        pos_ids = transform_to_id(pos, pos_class)

        print "generate labels"
        labels = [generate_labels_from_arcs(r) for r in trans]

        print "create training data"
        create_data_for_model(self.output_path, sentence_ids, pos_ids, arc_ids, labels)

def get_vocab_processor(emb_path, vocab_path, create_new=False):
    if create_new:
        vocab = load_vocab(emb_path)
        vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length=1)
        vocab_processor.fit(vocab)
        vocab_processor.save(vocab_path)
    else:
        vocab_processor = data_helper.get_vocab(vocab_path)
    return vocab_processor

def transform_sentences(sentenses, vocab_processor):
    sentence_ids = []
    for s in sentenses:
        ids = np.array(list(vocab_processor.transform(s))).flatten().tolist()
        sentence_ids.append(ids)
    return sentence_ids


def write_class(items, path):
    def _create_class_map(items):
        classes = list(set([e for l in items for e in l]))
        return {k: i for i, k in enumerate(classes)}

    class_map = _create_class_map(items)
    with open(path, 'w') as f:
        for k, v in class_map.items():
            f.write('%s,%s\n' %(v, k))
    return class_map


def transform_to_id(items, class_map):
    ids = []
    for l in items:
        ids.append([class_map[e] for e in l])
    return ids

def create_data_for_model(data_path, sentence_ids, pos_ids, arc_ids, labels):
    with open(data_path, 'w') as f:
        i = 0
        for sent, pos, arc, y in zip(sentence_ids, pos_ids, arc_ids, labels):
            parser = data_helper.Parser(len(sent))
            for action in y[2:-1]:   # skip first two 'shift' and last 'left-arc' actions
                parser.step(action)

                actions = [0]*3
                actions[action] = 1
                row = create_features(parser, sent, pos, arc) + actions
                # print row
                f.write(','.join(str(e) for e in row)+'\n')
            i += 1
            if i % 100 == 0:
                print "created data from %s sentenses" % (i)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--arc_class_path', type=str, default='data/arc_class.csv',
                        help='file storing arc class')
    parser.add_argument('--pos_class_path', type=str, default='data/pos_class.csv',
                        help='file storing pos class')
    parser.add_argument('--emb_path', type=str, default='data/en-cw.txt',
                        help='file storing embeddings')
    parser.add_argument('--vocab_path', type=str, default='data/vocab',
                        help='file storing vocabulary')
    parser.add_argument('--output_path', type=str, default='data/output.csv',
                        help='training data output path')
    parser.add_argument('--data_path', type=str, default='data/train.gold.conll',
                        help='input data for preprocessing')
    parser.add_argument('--train', default=False, action="store_true",
                        help='data is from trainset or not')
    args = parser.parse_args()

    proc = Preprocess(args.data_path, args.output_path, args.arc_class_path, 
        args.pos_class_path, args.emb_path, args.vocab_path, args.train)

    proc.run()








